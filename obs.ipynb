{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36baae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, Tensor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "815e8f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryota/whisper-finetune/env_whisper-finetune/lib/python3.10/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Path to your checkpoint\n",
    "checkpoint_path = \"output/custom_librispeech_test/checkpoint-490\"\n",
    "\n",
    "# Load the model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "07d50987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "\n",
      "WhisperForConditionalGeneration(\n",
      "  (model): WhisperModel(\n",
      "    (encoder): WhisperEncoder(\n",
      "      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (embed_positions): Embedding(1500, 384)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x WhisperEncoderLayer(\n",
      "          (self_attn): WhisperAttention(\n",
      "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): WhisperDecoder(\n",
      "      (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
      "      (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x WhisperDecoderLayer(\n",
      "          (self_attn): WhisperAttention(\n",
      "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): WhisperAttention(\n",
      "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print model structure\n",
    "print(\"Model structure:\\n\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea25f9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Module type                             Count\n",
      "-------------------------------------------------------\n",
      "Conv1d                                   2\n",
      "Embedding                                2\n",
      "GELUActivation                           8\n",
      "LayerNorm                                22\n",
      "Linear                                   65\n",
      "ModuleList                               2\n",
      "WhisperAttention                         12\n",
      "WhisperDecoder                           1\n",
      "WhisperDecoderLayer                      4\n",
      "WhisperEncoder                           1\n",
      "WhisperEncoderLayer                      4\n",
      "WhisperForConditionalGeneration          1\n",
      "WhisperModel                             1\n",
      "WhisperPositionalEmbedding               1\n"
     ]
    }
   ],
   "source": [
    "# Print module types\n",
    "module_names = defaultdict(int)\n",
    "\n",
    "# Iterating through all of the modules\n",
    "for name, module in model.named_modules():\n",
    "    module_names[type(module).__name__] += 1\n",
    "\n",
    "# Print cleanly\n",
    "print(\"\\nModule type\".ljust(40), \"Count\")\n",
    "print(\"-\" * 55)\n",
    "for module_type, count in sorted(module_names.items(), key=lambda x: x[0]):\n",
    "    print(f\"{module_type.ljust(40)} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "12887c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataclass to store the cache\n",
    "@dataclass\n",
    "class OBSLinearCache:\n",
    "    # Initilize empty cache\n",
    "    name: str = None\n",
    "    weight: Tensor = None\n",
    "    input: Tensor = None\n",
    "    output: Tensor = None\n",
    "    module: nn.Linear = None\n",
    "\n",
    "# Define a function to populate the cache\n",
    "def get_layer_hook(name: str):\n",
    "    # Create instance of cache\n",
    "    cache = OBSLinearCache()\n",
    "\n",
    "    def hook_fn(module, args, outputs):\n",
    "        # Update cache\n",
    "        cache.module = module\n",
    "        cache.name = name\n",
    "        cache.input = args\n",
    "        cache.output = outputs\n",
    "        if hasattr(module, \"weight\"):\n",
    "            cache.weight = module.weight\n",
    "\n",
    "    return hook_fn, cache        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b2d6b7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.conv1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.conv2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.embed_positions': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.self_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.self_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.self_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.self_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.self_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.self_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.activation_fn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.fc1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.fc2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.0.final_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.self_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.self_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.self_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.self_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.self_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.self_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.activation_fn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.fc1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.fc2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.1.final_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.self_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.self_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.self_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.self_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.self_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.self_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.activation_fn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.fc1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.fc2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.2.final_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.self_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.self_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.self_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.self_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.self_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.self_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.activation_fn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.fc1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.fc2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layers.3.final_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.encoder.layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.embed_tokens': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.embed_positions': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.self_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.self_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.self_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.self_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.self_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.activation_fn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.self_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.encoder_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.encoder_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.encoder_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.encoder_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.encoder_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.encoder_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.fc1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.fc2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.0.final_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.self_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.self_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.self_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.self_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.self_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.activation_fn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.self_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.encoder_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.encoder_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.encoder_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.encoder_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.encoder_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.encoder_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.fc1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.fc2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.1.final_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.self_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.self_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.self_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.self_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.self_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.activation_fn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.self_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.encoder_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.encoder_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.encoder_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.encoder_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.encoder_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.encoder_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.fc1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.fc2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.2.final_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.self_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.self_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.self_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.self_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.self_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.activation_fn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.self_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.encoder_attn': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.encoder_attn.k_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.encoder_attn.v_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.encoder_attn.q_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.encoder_attn.out_proj': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.encoder_attn_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.fc1': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.fc2': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layers.3.final_layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'model.decoder.layer_norm': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None), 'proj_out': OBSLinearCache(name=None, weight=None, input=None, output=None, module=None)}\n"
     ]
    }
   ],
   "source": [
    "# Create caches and hooks throughout the model\n",
    "caches = {}\n",
    "hooks = {}\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    hook_fn, cache = get_layer_hook(name)\n",
    "    caches[name] = cache\n",
    "    hooks[name] = module.register_forward_hook(hook_fn)\n",
    "\n",
    "print(caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward pass successful!\n",
      "Generated output shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input\n",
    "dummy_input_features = torch.randn(1, 80, 3000)  \n",
    "\n",
    "# Feed dummy input through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(dummy_input_features, max_length=448)\n",
    "print(\"Model forward pass successful!\")\n",
    "print(f\"Generated output shape: {outputs.shape}\")\n",
    "print(caches.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_whisper-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
